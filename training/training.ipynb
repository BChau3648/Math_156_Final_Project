{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import datetime as dt\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "\n",
    "# TODO: change path name\n",
    "sys.path.append(\"/home/bchau/Math_156_temp/Final_Project/preprocessing/\")\n",
    "from preprocessing import EuroSATDataset\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: change path name\n",
    "# setting paths to EuroSAT data and preprocessing statistics\n",
    "data_path = '/home/bchau/Math_156_temp/Final_Project/EuroSAT_RGB'\n",
    "preprocessing_stats_path = '/home/bchau/Math_156_temp/Final_Project/preprocessing/preprocessing_stats.pkl'\n",
    "checkpoint_path = '/home/bchau/Math_156_temp/Final_Project/checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting eurosat dataset\n",
    "eurosat = EuroSATDataset(data_path, preprocessing_stats_path, transform=True)\n",
    "classes = eurosat.sorted_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting dataset into train, validation, and test\n",
    "generator = torch.Generator().manual_seed(0)\n",
    "train_val_set, test_set = random_split(eurosat, [0.8, 0.2], generator = generator)\n",
    "train_set, val_set = random_split(train_val_set, [0.8, 0.2], generator = generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, 3)\n",
    "        self.pooling = nn.MaxPool2d(2, 2)\n",
    "        self.batch_norm = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x) \n",
    "        x = self.pooling(x)\n",
    "        x = self.batch_norm(x) \n",
    "        x = self.relu(x) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, is_output=False):\n",
    "        super().__init__()\n",
    "        self.is_output = is_output\n",
    "        self.conv = nn.Linear(in_channels, out_channels)\n",
    "        if not self.is_output:\n",
    "            self.batch_norm = nn.BatchNorm1d(out_channels)\n",
    "            self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if not self.is_output: \n",
    "            x = self.batch_norm(x)\n",
    "            x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv_block1 = ConvBlock(3, 8)\n",
    "        self.conv_block2 = ConvBlock(8, 12)\n",
    "        self.conv_block3 = ConvBlock(12, 16)\n",
    "        flatten_channels = 16 * 6 * 6\n",
    "        self.fc_block1 = FullyConnectedBlock(flatten_channels, flatten_channels // 2)\n",
    "        self.fc_block2 = FullyConnectedBlock(flatten_channels // 2, 10, is_output=True)\n",
    "        self.conv_blocks = nn.Sequential(\n",
    "            self.conv_block1, \n",
    "            self.conv_block2, \n",
    "            self.conv_block3\n",
    "        )\n",
    "        self.fc_blocks = nn.Sequential(\n",
    "            self.fc_block1, \n",
    "            self.fc_block2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_blocks(x)\n",
    "        #  flatten all dimensions except batch\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc_blocks(x)\n",
    "        return x\n",
    "    \n",
    "test_model = Net()\n",
    "test_img = torch.rand((10, 3, 64, 64))\n",
    "test_model(test_img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing model\n",
    "model_name = 'cnn' # name of model (for checkpoint file name)\n",
    "model = Net().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: setting hyperparameters\n",
    "batch_size = 64\n",
    "epochs = 20 \n",
    "optimizer = torch.optim.SGD(model.parameters())\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# containers for storing loss data and epoch times\n",
    "train_loss = []\n",
    "train_loss_idx = []\n",
    "val_loss = []\n",
    "val_loss_idx = []\n",
    "epoch_times = []\n",
    "\n",
    "# tracking when to checkpoint model\n",
    "checkpoint_after_epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataloaders\n",
    "train_loader = DataLoader(train_set, batch_size = batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_set, batch_size = batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_set, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 64, 64])\n",
      "torch.float32\n",
      "torch.Size([64])\n",
      "torch.int64\n",
      "torch.float32\n",
      "tensor(2.4901, device='cuda:0', grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Testing loss\n",
    "for data in train_loader:\n",
    "    imgs = data['image'].to(device)\n",
    "    labels = data['land_use'].to(device)\n",
    "    print(imgs.shape)\n",
    "    print(imgs.dtype)\n",
    "    print(labels.shape)\n",
    "    print(labels.dtype)\n",
    "    test_output = model(imgs)\n",
    "    print(test_output.dtype)\n",
    "    print(torch.nn.CrossEntropyLoss()(test_output, labels))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(epoch_index, optimizer, loss_fn, train_loader, model, \n",
    "                    train_loss, train_loss_idx):\n",
    "    running_loss = 0.\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in enumerate(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs = data['image'].to(device)\n",
    "        labels = data['land_use'].to(device)\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item().detach()\n",
    "        if i % 90 == 89:\n",
    "            last_loss = running_loss / 90 # loss per batch\n",
    "            timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "            print('{} batch {} loss: {}'.format(timestamp, i + 1, last_loss))\n",
    "            train_loss_idx.append(epoch_index * len(train_loader) + i + 1)\n",
    "            train_loss.append(last_loss)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(epoch, optimizer, loss_fn, model, \n",
    "               train_loss, train_loss_idx, val_loss, val_loss_idx, status):\n",
    "    model_path = os.path.join(checkpoint_path, f'{status}_{model_name}_e{epoch}.tar')\n",
    "    result = {\n",
    "        'epoch': epoch, \n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_fn': loss_fn, \n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'train_loss': train_loss, \n",
    "        'train_loss_idx': train_loss_idx, \n",
    "        'val_loss': val_loss, \n",
    "        'val_loss_idx': val_loss_idx\n",
    "    }\n",
    "    timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "    print(f'{timestamp} Saving results at {checkpoint_path}')\n",
    "    torch.save(result, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, model_type, optimizer_type=None):\n",
    "    checkpoint = torch.load(checkpoint_path, weights_only=False)\n",
    "    model_type.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # returns all training information if optimizer is provided\n",
    "    if optimizer_type:\n",
    "        optimizer_type.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        model_epoch = checkpoint['epoch']\n",
    "        loss_fn = checkpoint['loss_fn']\n",
    "        train_loss = checkpoint['train_loss']\n",
    "        train_loss_idx = checkpoint['train_loss_idx']\n",
    "        val_loss = checkpoint['val_loss']\n",
    "        val_loss_idx = checkpoint['val_loss_idx']\n",
    "        return (model_epoch, optimizer_type, loss_fn, model_type, \n",
    "                train_loss, train_loss_idx, val_loss, val_loss_idx)\n",
    "    # otherwise only return model\n",
    "    return model_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(epochs, optimizer, loss_fn, train_loader, model):\n",
    "    best_vloss = torch.inf \n",
    "    for epoch in range(epochs):\n",
    "        timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "        print(f\"{timestamp} Epoch {epoch}/{epochs}\")\n",
    "\n",
    "        # Make sure gradient tracking is on, and do a pass over the data\n",
    "        model.train(True)\n",
    "        epoch_start_time = time.time()\n",
    "        avg_loss = train_one_epoch(epoch, optimizer, loss_fn, train_loader, model, train_loss, train_loss_idx)\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_times.append(epoch_end_time - epoch_start_time)\n",
    "\n",
    "        timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "        print(f\"{timestamp} Finished training in {str(dt.timedelta(seconds = epoch_times[-1]))}\")\n",
    "\n",
    "        # Set the model to evaluation mode, disabling dropout and using population\n",
    "        # statistics for batch normalization.\n",
    "        model.eval()\n",
    "\n",
    "        running_vloss = 0.0\n",
    "        # Disable gradient computation and reduce memory consumption.\n",
    "        with torch.no_grad():\n",
    "            for i, vdata in enumerate(val_loader):\n",
    "                vinputs = vdata['image'].to(device)\n",
    "                vlabels = vdata['land_use'].to(device)\n",
    "                voutputs = model(vinputs)\n",
    "                vloss = loss_fn(voutputs, vlabels)\n",
    "                running_vloss += vloss.item().detach()\n",
    "\n",
    "        avg_vloss = running_vloss / (i + 1)\n",
    "        timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "        print('{} LOSS train {} valid {}'.format(timestamp, avg_loss, avg_vloss))\n",
    "\n",
    "        # Log the validation running loss averaged per batch\n",
    "        val_loss_idx.append(len(train_loader) * (epoch + 1))\n",
    "        val_loss.append(avg_vloss)\n",
    "\n",
    "        # Track best performance, and save the model's state\n",
    "        if avg_vloss < best_vloss:\n",
    "            best_vloss = avg_vloss\n",
    "            timestamp = dt.datetime.now().strftime('%Y-%m-%d %H-%M-%S')\n",
    "            print(f\"{timestamp} New best validation loss: {best_vloss}\")\n",
    "            save_model(epoch + 1, optimizer, loss_fn, model, \n",
    "                       train_loss, train_loss_idx, val_loss, val_loss_idx, 'best')\n",
    "        elif epoch % checkpoint_after_epochs == checkpoint_after_epochs - 1:\n",
    "            save_model(epoch + 1, optimizer, loss_fn, model, \n",
    "                       train_loss, train_loss_idx, val_loss, val_loss_idx, 'latest')\n",
    "        \n",
    "        print('=================================')\n",
    "\n",
    "    save_model(epoch + 1, optimizer, loss_fn, model, \n",
    "               train_loss, train_loss_idx, val_loss, val_loss_idx, 'latest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-09 17-29-09 Epoch 0/20\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[33], line 10\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(epochs, optimizer, loss_fn, train_loader, model)\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 10\u001b[0m avg_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loss_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m epoch_end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     12\u001b[0m epoch_times\u001b[38;5;241m.\u001b[39mappend(epoch_end_time \u001b[38;5;241m-\u001b[39m epoch_start_time)\n",
      "Cell \u001b[0;32mIn[13], line 26\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(epoch_index, optimizer, loss_fn, train_loader, model, train_loss, train_loss_idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Gather data and report\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m90\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m89\u001b[39m:\n\u001b[1;32m     28\u001b[0m     last_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m90\u001b[39m \u001b[38;5;66;03m# loss per batch\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "train_model(epochs, optimizer, loss_fn, train_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(data_loader, model):\n",
    "    all_labels = torch.zeros(len(data_loader.dataset))\n",
    "    all_predictions = torch.zeros(len(data_loader.dataset))\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(tqdm(data_loader)):\n",
    "            inputs = data['image'].to(device)\n",
    "            labels = data['land_use'].cpu()\n",
    "            outputs = model(inputs)\n",
    "            predictions = torch.argmax(outputs, dim=1).cpu()\n",
    "            start_idx = i * data_loader.batch_size\n",
    "            if i != len(data_loader) - 1:\n",
    "                end_idx = start_idx + data_loader.batch_size\n",
    "            else:\n",
    "                end_idx = start_idx + len(labels)\n",
    "                assert(end_idx == len(data_loader.dataset))\n",
    "            all_labels[start_idx:end_idx] = labels\n",
    "            all_predictions[start_idx:end_idx] = predictions \n",
    "\n",
    "    metrics = pd.DataFrame(columns=['Precision', 'Recall', 'F1_Score'])\n",
    "    \n",
    "    micro_avg = precision_recall_fscore_support(all_labels, all_predictions, \n",
    "                                                average='micro', zero_division='warn')\n",
    "    macro_avg = precision_recall_fscore_support(all_labels, all_predictions, \n",
    "                                                average='macro', zero_division='warn')\n",
    "    accuracy = torch.sum(all_labels == all_predictions) / len(all_labels)\n",
    "    metrics.loc['Micro Avg'] = micro_avg[:3]\n",
    "    metrics.loc['Macro Avg'] = macro_avg[:3]\n",
    "    metrics.loc['Accuracy'] = [accuracy.item(), None, None]\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest_model_path = os.path.join(checkpoint_path, 'first_run', 'latest_cnn_e19')\n",
    "checkpoint_model = Net()\n",
    "checkpoint_model = load_model(latest_model_path, checkpoint_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/85 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85/85 [00:20<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "metrics = test_model(test_loader, checkpoint_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Micro Avg</th>\n",
       "      <td>0.774444</td>\n",
       "      <td>0.774444</td>\n",
       "      <td>0.774444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Macro Avg</th>\n",
       "      <td>0.765669</td>\n",
       "      <td>0.768775</td>\n",
       "      <td>0.764691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.774444</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Precision    Recall  F1_Score\n",
       "Micro Avg   0.774444  0.774444  0.774444\n",
       "Macro Avg   0.765669  0.768775  0.764691\n",
       "Accuracy    0.774444       NaN       NaN"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: grid search specific to CNN, googlenet, mobilenet"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "math_156_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
